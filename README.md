# Transcriber
- Uses various [`openai-whisper`](https://github.com/openai/whisper) based models to transcribe audio or video files.
- Runs locally.
- Outputs into a clean .cha file by utterance.
- As always, TRUST NOTHING GENERATED BY AI, and always verify

## Requirements:
- if you can, use cuda. https://pytorch.org/get-started/locally/ _do this before running the pip install requirements_
- Python3+
- internet (for model download, on the first time you use it)

---
# How to use:
1. install requirements
    - `pip install -r requirements.txt`
    - Follow the guide here if you have a CUDA compatible GPU: https://docs.nvidia.com/cuda/cuda-quick-start-guide/
        - You may need to `pip uninstall torch torchaudio torchvision` before doing this step.
1. Put your [Huggingface token](https://huggingface.co/docs/hub/en/security-tokens) in a file named `.hftoken`. (Note that it does not need any special permissions, you can deselect all of the options.)
1. Start application.
    - `python ./transcribe.py`
1. Select files (button) to be transcribed.
    - Set the number of speakers in the number box.
    - Set the language (if not english).
1. Select AI model to use from the drop down.
    - Hover over the drop down to see some selection guidance. Choose one that your device can handle.
    - [Read more about the models here.](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages)
4. Start transcript (button).
5. Wait for final results popup to appear.
6. Review resulting transcripts.

---
### Backlog ideas:
- [ ] Advanced/Runtime configuration of AI parameters?
- [ ] Bundle into single executable to be more user friendly?
- [ ] Select subframe of time to transcribe from?

If we use huggingface:
see here for enabling symbolic links:
`gpedit.msc` -> 
![alt text](docs\readme_hf_allow_symlink.png)
@todo show how you can add yourself to create symbolic links


