# Transcriber
- Uses [`openai-whisper`](https://github.com/openai/whisper) to transcribe audio or video files.
- Runs locally.
- Outputs into a clean workbook by trascribed section and word. 
- As always, TRUST NOTHING GENERATED BY AI, and always verify

## Requirements:
- Python3+
- ffmpeg (included in repo)
- beefy pc(?)
- internet (for model download, on the first time you use it)
---
# How to use:
0. install requirements
    - `pip install -r requirements.txt`
1. Start application.
    - `python ./transcribe.py`
2. Select files (button) to be transcribed.
3. Select AI model to use from the drop down.
    - Hover over the drop down to see some selection guidance. Choose one that your device can handle.
    - [Read more about the models here.](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages)
4. Start transcript (button).
5. Wait for final results popup to appear.
6. Review resulting transcripts.
---
#### What the running program looks like.
![What the running program looks like.](docs\readme_demo.png)
#### Completion notification.
![Completion notification.](docs\readme_transcripts_complete.png)
#### Output files can be found next to the input files.
![Output files can be found next to the input files.](docs\readme_output_location.png)
#### Output contents is 2 sheets, one by segment and one by word.
![Output contents is 2 sheets, one by segment and one by word.](docs\readme_demo_outputs.png)
##### Don't blame me if the AI is bad. (Same input as previous pic, but got different output.)
![Same input but different output](docs\readme_same_input_different_output.png)

---
### Backlog ideas:
- [ ] Additional AI sources?
- [ ] Advanced/Runtime configuration of AI parameters?
- [ ] Bundle into single executable to be more user friendly?
- [ ] Select subframe of time to transcribe from?

If we use huggingface:
see here for enabling symbolic links:
`gpedit.msc` -> 
![alt text](docs\readme_hf_allow_symlink.png)
@todo show how you can add yourself to create symbolic links


